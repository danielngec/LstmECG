{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tezte \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/lar-desktop-ur5/Área de Trabalho/danielufba/icdaniel/mitbih_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1036\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1090\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1165\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Área de Trabalho/danielufba/icdaniel/lib/python3.8/site-packages/pandas/core/dtypes/common.py:1335\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1331\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1332\u001b[0m     )\n\u001b[0;32m-> 1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tezte = pd.read_csv('/home/lar-desktop-ur5/Área de Trabalho/danielufba/icdaniel/mitbih_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_em_segments(dataset, tamanho_segmento): # Função para dividir o dataset em segmentos com um número fixo de colunas (tamanho_segmento).\n",
    "\n",
    "    segmentos = []\n",
    "    distancia = dataset.shape[1] - tamanho_segmento + 1\n",
    "\n",
    "    for i in range(dataset.shape[0]):\n",
    "        for j in range(distancia):\n",
    "            segmentos.append(dataset.iloc[i, j:(j + tamanho_segmento)].values)\n",
    "\n",
    "    dataset_segmentado = pd.DataFrame(segmentos)  # Criar o DataFrame com os segmentos\n",
    "    return dataset_segmentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preenche_zeros_com_valor(dataset): # Função para substituir os zeros nas últimas posições de cada linha com NaN.\n",
    "    for i in range(dataset.shape[0]):\n",
    "        for j in range(dataset.shape[1]-1):\n",
    "            if dataset.iloc[i, -(j+1)] == 0:\n",
    "                dataset.iloc[i, -(j+1)] = np.nan\n",
    "            else:\n",
    "                break\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessamento(dataset, tamanho_segmento):\n",
    "    dataset = preenche_zeros_com_valor(dataset)\n",
    "    dataset = divide_em_segments(dataset, tamanho_segmento)\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tezte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tezte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novos_nomes = [f'col{i+1}' for i in range(tezte.shape[1])]\n",
    "novos_nomes[-1] = 'target'\n",
    "tezte.columns = novos_nomes\n",
    "tezte.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n = tezte[tezte['target'] == 0]\n",
    "dataset_n = dataset_n.drop('target', axis=1)\n",
    "datasetnormal, datasetnormal2 = train_test_split(dataset_n, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetnormal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"training_log.txt\"\n",
    "\n",
    "# Apagar o conteúdo do arquivo e adicionar uma nova mensagem\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(\"Iniciou o Processo 2 2\\n\")\n",
    "\n",
    "# Enviar a mensagem para o tmux\n",
    "os.system(f'tmux send-keys -t danieltrain \"echo \\\\\"Iniciou o Processo\\\\\"\" C-m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_segmentando = preprocessamento(datasetnormal, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"training_log.txt\"\n",
    "with open(log_file, \"a\") as log:\n",
    "    log.write(\"Terminou de fazer o dataset\\n\")\n",
    "\n",
    "os.system(f'tmux send-keys -t danieltrain \"echo \\\\\"Terminou o Dataset\\\\\"\" C-m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "dataset_segmentando_scaled = scaler.fit_transform(dataset_segmentando)\n",
    "dataset_segmentando_scaled = pd.DataFrame(dataset_segmentando_scaled, columns=dataset_segmentando.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset_segmentando_scaled.iloc[:, :-1]\n",
    "y = dataset_segmentando_scaled.iloc[:, -1]\n",
    "xsegmentando = np.array(x)\n",
    "ysegmentando = np.array(y)\n",
    "xsegmentando = xsegmentando.reshape((xsegmentando.shape[0], xsegmentando.shape[1], 1))\n",
    "ysegmentando = ysegmentando.reshape(-1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (xsegmentando.shape)\n",
    "print (ysegmentando.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import LambdaCallback,ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "# Nome do arquivo de log\n",
    "log_file = \"training_log.txt\"\n",
    "\n",
    "# Callback para salvar no início do treinamento e no início de cada época\n",
    "training_start_callback = LambdaCallback(\n",
    "    on_train_begin=lambda logs: (\n",
    "        open(log_file, \"a\").write(f\" Treinamento iniciado!\\n\"),\n",
    "        os.system(f'tmux send-keys -t danieltrain \"Treinamento iniciado!\" C-m')\n",
    "    ),\n",
    "    on_epoch_begin=lambda epoch, logs: (\n",
    "        open(log_file, \"a\").write(f\" Início da Epoch {epoch+1} {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"),\n",
    "        os.system(f'tmux send-keys -t danieltrain \" Início da Epoch {epoch+1} \" C-m')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Callback para salvar no final de cada época\n",
    "tmux_callback = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: (\n",
    "        # Salvar no arquivo de log\n",
    "        open(log_file, \"a\").write(\n",
    "            f\" Epoch {epoch+1} - Loss: {logs['loss']:.4f} - Val Loss: {logs['val_loss']:.4f}\\n\"\n",
    "        ),\n",
    "        # Enviar para o tmux\n",
    "        os.system(\n",
    "            f'tmux send-keys -t danieltrain \" Epoch {epoch+1} - Loss: {logs[\"loss\"]:.4f} - Val Loss: {logs[\"val_loss\"]:.4f}\" C-m'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Callback para salvar o melhor modelo\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='model_checkpoint6.h5', \n",
    "    save_best_only=False,            \n",
    "    save_weights_only=False,        \n",
    "    verbose=1                        \n",
    ")\n",
    "\n",
    "# Callback para parar o treinamento cedo se não houver melhora\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    \n",
    "    patience=10,           \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_24 (LSTM)              (None, 9, 128)            66560     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 9, 128)            0         \n",
      "                                                                 \n",
      " lstm_25 (LSTM)              (None, 9, 128)            131584    \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 9, 128)            0         \n",
      "                                                                 \n",
      " lstm_26 (LSTM)              (None, 9, 64)             49408     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 9, 64)             0         \n",
      "                                                                 \n",
      " lstm_27 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 281,616\n",
      "Trainable params: 281,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "   496/103571 [..............................] - ETA: 26:07 - loss: 0.2451    "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(xsegmentando.shape[1], xsegmentando.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(16))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()\n",
    "history = model.fit(xsegmentando, ysegmentando, epochs=50, batch_size=32, validation_split=0.1, verbose=1, callbacks=[training_start_callback, tmux_callback, checkpoint_callback, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_file, \"a\") as log:\n",
    "    log.write(\"Treinamento Terminou\\n\")\n",
    "\n",
    "os.system(f'tmux send-keys -t danieltrain \"echo \\\\\"Terminou o Treinamento\\\\\"\" C-m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevteste = model.predict(xsegmentando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m erros \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mabs(prevteste \u001b[38;5;241m-\u001b[39m ysegmentando)\n\u001b[1;32m      3\u001b[0m mae_vetores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(erros, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m mae_geral \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(erros)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "erros = np.abs(prevteste - ysegmentando)\n",
    "\n",
    "mae_vetores = np.mean(erros, axis=1)\n",
    "\n",
    "mae_geral = np.mean(erros)\n",
    "\n",
    "vetores_maiores = mae_vetores > mae_geral\n",
    "quantidade_vetores_maiores = np.sum(vetores_maiores)\n",
    "quantidade_vetores = len(mae_vetores)\n",
    "\n",
    "print(\"MAE de cada vetor:\", mae_vetores)\n",
    "print(\"MAE geral:\", mae_geral)\n",
    "print(\"Quantidade de vetores com erro médio maior que o MAE geral:\", quantidade_vetores_maiores)\n",
    "print(\"Quantidade de vetores :\", quantidade_vetores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(mae_vetores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros[0].sum()/len(erros[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "for i in range(len(mae_vetores)):\n",
    "    if mae_vetores[i]>mae_geral:\n",
    "        lista.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 68164\n",
    "print(\"Preditor:\",xsegmentando[n])\n",
    "print(\"predição:\",prevteste[n])\n",
    "print(\"Valor Real:\",ysegmentando[n])\n",
    "print(\"Erro:\",mae_vetores[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_maiores_que_2 = np.where(mae_vetores > 2)[0]\n",
    "\n",
    "print(\"Índices onde os valores são maiores que 2:\", indices_maiores_que_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (icdaniel)",
   "language": "python",
   "name": "icdaniel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
